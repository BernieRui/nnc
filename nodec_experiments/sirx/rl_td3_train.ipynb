{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIRX: Training RL\n",
    "\n",
    "Baseline comparion in terms of total loss and energy.\n",
    "\n",
    "To run this script:\n",
    "1. Please make sure that the required data folder is available at the paths used by the script.\n",
    "You may generate the required data by running the python script\n",
    "```nodec_experiments/sirx/gen_parameters.py```.\n",
    "\n",
    "2. The scripts below:\n",
    " - ```nodec_experiments/sirx/sirx.py```\n",
    " - ```nodec_experiments/sirx/rl_utils.py```\n",
    " - ```nodec_experiments/sirx/sirx_utils.py```\n",
    "contain very important utilities for running training , evaluation and plotting scripts. Please make sure that they are available in the python path when running experiments.\n",
    "\n",
    "Reinforcement Learning requires some significant time to train.\n",
    "\n",
    "As neural network intialization is stochastic, please make sure that appropriate seeds are used or expect some variance to paper results.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\") # append modules from parent dir\n",
    "\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "from nnc.controllers.neural_network.nnc_controllers import NNCDynamics\n",
    "from nnc.helpers.torch_utils.graphs import drivers_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sirx import SIRDelta, neighborhood_mask, flat_to_channels, GCNNControl\n",
    "from rl_utils import SIRXEnv, RLGCNN, Actor, Critic\n",
    "\n",
    "import tianshou as ts\n",
    "from tianshou.policy import TD3Policy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.data import Collector, ReplayBuffer, to_torch\n",
    "from tianshou.exploration import GaussianNoise\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = 'lattice'\n",
    "parameters_folder = '../../../data/parameters/sirx/'\n",
    "results_folder = '../../../results/sirx/'+graph+'/'\n",
    "\n",
    "graph_parameters_folder = parameters_folder + '/' + 'lattice' + '/'\n",
    "\n",
    "adjacency_matrix = torch.load(graph_parameters_folder + 'adjacency.pt', map_location=device).to(dtype)\n",
    "n_nodes = adjacency_matrix.shape[-1]\n",
    "drivers = torch.load(graph_parameters_folder + 'drivers.pt', map_location='cpu').to(torch.long)\n",
    "driver_matrix = drivers_to_tensor(n_nodes, drivers).to(dtype=dtype, device=device)\n",
    "alpha = adjacency_matrix\n",
    "beta = driver_matrix\n",
    "side_size = int(np.sqrt(n_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamics Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.load(graph_parameters_folder + 'initial_state.pt').to(device=device, dtype=dtype)\n",
    "target_subgraph = torch.load(graph_parameters_folder + 'target_subgraph_nodes.pt')\n",
    "dynamics_params = torch.load(graph_parameters_folder + 'dynamics_parameters.pt')\n",
    "# budget and rates need to be choosen according to graph size\n",
    "budget = dynamics_params['budget']\n",
    "infection_rate = dynamics_params['infection_rate']\n",
    "recovery_rate = dynamics_params['recovery_rate']\n",
    "total_time = 5 # determined via no control testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sirx_dyn = SIRDelta(\n",
    "             adjacency_matrix=alpha,\n",
    "             infection_rate=infection_rate,\n",
    "             recovery_rate=recovery_rate,\n",
    "             driver_matrix=beta,\n",
    "             k_0=0.0,\n",
    "            ).to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_dt = 0.01 # RL interaction frequency\n",
    "env_config={\n",
    "    'sirx' : sirx_dyn,\n",
    "    'target_nodes' : target_subgraph.tolist(),\n",
    "    'dt' : rl_dt,\n",
    "    'T' : total_time,\n",
    "    'ode_solve_method' : 'dopri5',\n",
    "    'reward_type' : 'sum_to_max',\n",
    "    'x0' : x0,\n",
    "    'budget' : budget    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([lambda: SIRXEnv(env_config) for _ in range(2)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: SIRXEnv(env_config) for _ in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Neural Networks\n",
    "If you check code you will see that it has the same learnable parameters and structure as the network\n",
    "used for NODEC before the decision layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, ninds = neighborhood_mask(alpha)\n",
    "in_preprocessor = lambda x: flat_to_channels(x, n_nodes=n_nodes, mask=mask, inds=ninds)\n",
    "\n",
    "policy_net = RLGCNN(\n",
    "                   adjacency_matrix = alpha,\n",
    "                   driver_matrix = beta, \n",
    "                   input_preprocessor = in_preprocessor,\n",
    "                   in_channels=4,\n",
    "                   feat_channels=5,\n",
    "                   message_passes=4\n",
    "                  )\n",
    "\n",
    "actor = Actor(model = policy_net, device=device).to(device)\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=0.0003)\n",
    "\n",
    "critic1 = Critic(1, 4096, 512, device=device).to(device)\n",
    "critic1_optim = torch.optim.Adam(critic1.parameters(), lr=1e-4)\n",
    "\n",
    "critic2 = Critic(1, 4096, 512, device=device).to(device)\n",
    "critic2_optim = torch.optim.Adam(critic2.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for transfer learning we can literally load the model\n",
    "#actor.model.load_state_dict(torch.load('../sir/sirx_best.torch'))\n",
    "secs = int(round(time.time()))\n",
    "log_path = results_folder + 'rl/td3/time_'+str(secs)\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Policy training proceedure\n",
    "# evaluation environment\n",
    "env = SIRXEnv(env_config)\n",
    "\n",
    "\n",
    "# YOu can change TD3 to SAC or any other contious action policy provided from tianshou\n",
    "policy = TD3Policy(\n",
    "    actor = actor,\n",
    "    actor_optim = actor_optim,\n",
    "    critic1 = critic1,\n",
    "    critic1_optim = critic1_optim,\n",
    "    critic2 = critic2,\n",
    "    critic2_optim = critic2_optim,\n",
    "    tau= 0.005,\n",
    "    gamma = 0.999,\n",
    "    exploration_noise = GaussianNoise(0.01),\n",
    "    policy_noise = 0.001,\n",
    "    update_actor_freq = 5,\n",
    "    noise_clip = 0.5,\n",
    "    action_range =  [env.action_space.low[0], env.action_space.high[0]],\n",
    "    reward_normalization = True,\n",
    "    ignore_done = False,\n",
    ")\n",
    "\n",
    "\n",
    "   \n",
    "# Experience Collector\n",
    "train_collector = Collector(\n",
    "    policy, train_envs, ReplayBuffer(8000))\n",
    "test_collector = Collector(policy, test_envs)\n",
    "writer = SummaryWriter(log_path)\n",
    "\n",
    "def save_fn(policy):\n",
    "    # save best model\n",
    "    torch.save(policy.state_dict(), os.path.join(log_path, 'policy.pth'))\n",
    "\n",
    "# trainer\n",
    "result = offpolicy_trainer(\n",
    "    policy = policy,\n",
    "    train_collector = train_collector,\n",
    "    test_collector = test_collector,\n",
    "    max_epoch = 100,\n",
    "    step_per_epoch = len(env.time_steps),\n",
    "    collect_per_step = 1,\n",
    "    episode_per_test = 1,\n",
    "    batch_size = len(env.time_steps),\n",
    "    save_fn = save_fn,\n",
    "    writer = writer,\n",
    "    log_interval = 1,\n",
    "    verbose = True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
